{
  "model": {
    "version": "mlx-small-24b-8bit",
    "available_models": {
      "full": {
        "id": "mistralai/Voxtral-Mini-3B-2507",
        "name": "Voxtral Mini 3B (Full)",
        "size_gb": 9.36,
        "format": "safetensors",
        "quantization": null,
        "backend": "voxtral",
        "description": "Full precision model - Best quality, requires ~20-30GB RAM during loading (CPU loading: 10-30 min)",
        "memory_requirements": {
          "disk": "9.36 GB",
          "ram_loading": "20-30 GB (Windows: increase pagefile to 50GB)",
          "ram_inference": "10-12 GB",
          "load_time_cpu": "10-30 minutes (use NVIDIA GPU for faster loading)"
        }
      },
      "quantized": {
        "id": "mistralai/Voxtral-Mini-3B-2507",
        "name": "Voxtral Mini 3B (4-bit Quantized)",
        "size_gb": 9.36,
        "format": "safetensors",
        "quantization": "4bit",
        "backend": "voxtral",
        "description": "4-bit quantized model - 75% less memory usage (NVIDIA GPU required, auto-falls back to full precision on Mac/CPU)",
        "memory_requirements": {
          "disk": "9.36 GB (quantized during load)",
          "ram_loading": "5-8 GB (NVIDIA GPU) / 20-30GB (Mac/CPU fallback)",
          "ram_inference": "3-4 GB (NVIDIA GPU) / 10-12GB (Mac/CPU fallback)"
        }
      },
      "small-24b": {
        "id": "mistralai/Voxtral-Small-24B-2507",
        "name": "Voxtral Small 24B (Full)",
        "size_gb": 97.1,
        "format": "safetensors",
        "quantization": null,
        "backend": "voxtral",
        "description": "Large 24B model - Best quality and accuracy, requires high-end GPU with ~55GB VRAM or significant RAM",
        "memory_requirements": {
          "disk": "97.1 GB",
          "ram_loading": "55+ GB (GPU) / 100+ GB (CPU)",
          "ram_inference": "55 GB (GPU)",
          "load_time_cpu": "Not recommended - use GPU with 55+ GB VRAM"
        }
      },
      "small-24b-quantized": {
        "id": "mistralai/Voxtral-Small-24B-2507",
        "name": "Voxtral Small 24B (4-bit Quantized)",
        "size_gb": 97.1,
        "format": "safetensors",
        "quantization": "4bit",
        "backend": "voxtral",
        "description": "4-bit quantized 24B model - High quality with reduced memory, requires NVIDIA GPU with ~16GB VRAM",
        "memory_requirements": {
          "disk": "97.1 GB (quantized during load)",
          "ram_loading": "16-20 GB (NVIDIA GPU)",
          "ram_inference": "14-16 GB (NVIDIA GPU)",
          "load_time_cpu": "Not supported - requires NVIDIA GPU with CUDA"
        }
      },
      "mlx-mini-3b-4bit": {
        "id": "mzbac/voxtral-mini-3b-4bit-mixed",
        "name": "Voxtral Mini 3B MLX (4-bit)",
        "size_gb": 3.2,
        "format": "mlx",
        "quantization": "4bit",
        "backend": "mlx",
        "platform": "mac",
        "description": "MLX-optimized 4-bit model for Apple Silicon - Fast inference on Mac M1/M2/M3/M4 (25-30x faster than PyTorch)",
        "memory_requirements": {
          "disk": "3.2 GB",
          "ram_loading": "4-5 GB unified memory",
          "ram_inference": "4-5 GB unified memory",
          "load_time": "< 1 minute on Apple Silicon"
        }
      },
      "mlx-mini-3b-8bit": {
        "id": "mzbac/voxtral-mini-3b-8bit",
        "name": "Voxtral Mini 3B MLX (8-bit)",
        "size_gb": 5.3,
        "format": "mlx",
        "quantization": "8bit",
        "backend": "mlx",
        "platform": "mac",
        "description": "MLX-optimized 8-bit model for Apple Silicon - Better quality, fast inference on Mac M1/M2/M3/M4",
        "memory_requirements": {
          "disk": "5.3 GB",
          "ram_loading": "6-7 GB unified memory",
          "ram_inference": "6-7 GB unified memory",
          "load_time": "< 2 minutes on Apple Silicon"
        }
      },
      "mlx-small-24b-8bit": {
        "id": "mzbac/Voxtral-Small-24B-2507-8bit",
        "name": "Voxtral Small 24B MLX (8-bit)",
        "size_gb": 25.0,
        "format": "mlx",
        "quantization": "8bit",
        "backend": "mlx",
        "platform": "mac",
        "description": "MLX-optimized 24B model (8-bit) for Apple Silicon - Best quality, requires 64GB+ unified memory",
        "memory_requirements": {
          "disk": "25 GB",
          "ram_loading": "50-60 GB unified memory",
          "ram_inference": "45-55 GB unified memory",
          "load_time": "5-10 minutes on Apple Silicon"
        }
      }
    }
  },
  "app": {
    "version": "1.1.13",
    "name": "Voxtral Transcription"
  }
}