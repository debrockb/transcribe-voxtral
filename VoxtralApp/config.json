{
  "model": {
    "version": "full",
    "available_models": {
      "full": {
        "id": "mistralai/Voxtral-Mini-3B-2507",
        "name": "Voxtral Mini 3B (Full)",
        "size_gb": 9.36,
        "format": "safetensors",
        "quantization": null,
        "description": "Full precision model - Best quality, requires ~20-30GB RAM during loading (CPU loading: 10-30 min)",
        "memory_requirements": {
          "disk": "9.36 GB",
          "ram_loading": "20-30 GB (Windows: increase pagefile to 50GB)",
          "ram_inference": "10-12 GB",
          "load_time_cpu": "10-30 minutes (use NVIDIA GPU for faster loading)"
        }
      },
      "quantized": {
        "id": "mistralai/Voxtral-Mini-3B-2507",
        "name": "Voxtral Mini 3B (4-bit Quantized)",
        "size_gb": 9.36,
        "format": "safetensors",
        "quantization": "4bit",
        "description": "4-bit quantized model - 75% less memory usage (NVIDIA GPU required, auto-falls back to full precision on Mac/CPU)",
        "memory_requirements": {
          "disk": "9.36 GB (quantized during load)",
          "ram_loading": "5-8 GB (NVIDIA GPU) / 20-30GB (Mac/CPU fallback)",
          "ram_inference": "3-4 GB (NVIDIA GPU) / 10-12GB (Mac/CPU fallback)"
        }
      },
      "mlx-mini-3b-4bit": {
        "id": "mzbac/voxtral-mini-3b-4bit-mixed",
        "name": "Voxtral Mini 3B MLX (4-bit)",
        "backend": "mlx",
        "platform": "mac",
        "quantization": "4bit",
        "description": "MLX-optimized 4-bit model for Apple Silicon - Fast inference on Mac M1/M2/M3/M4"
      },
      "mlx-mini-3b-8bit": {
        "id": "mzbac/voxtral-mini-3b-8bit",
        "name": "Voxtral Mini 3B MLX (8-bit)",
        "backend": "mlx",
        "platform": "mac",
        "quantization": "8bit",
        "description": "MLX-optimized 8-bit model for Apple Silicon - Better quality, fast inference on Mac M1/M2/M3/M4"
      }
    }
  },
  "app": {
    "version": "1.1.13",
    "name": "Voxtral Transcription"
  }
}